# Sidekick
<img length="600" width="400"  src="https://github.com/user-attachments/assets/19d62006-ed38-46c1-8519-38cfb554110c" />

Idea is pretty simple: Being able to utilize a Multimodal LLM to the fullest. From taking screenshots to being able to converse with a language model with voice and making it Domain specifc based on your needs. From being a nifty assignment helper to a Junior Analyst giving you useful insights!
working on linking it up with ollama for you open source baddies. (P.S. A better looking ReadME file will be written soon. I am a little caught up working on the project :))
DISCLAIMER: THIS IS NOT A CHEATING TOOL AND I WILL NOT BE RELEASING AN INVISIBLE MODE. 
Current Demo: Everything in it including the system prompt is subject to change, The Main purpose of this tool remains TO be a personalized AI assistant with completely local LLMs. 

https://github.com/user-attachments/assets/60b3c47c-2d09-4fed-84ad-27491f5ba557

I MEANT VOICECHAT NOT VIDEOCHAT*

CURRENT TECH STACK:
Electron and Node Js for frontend
Python+ Flask API Backend
Crew AI
Gemini API
RealTimeSTT and RealTimeTTS + Whisper Models
Working on Ollama Integration- Preferably the main focus of the project in the future

General Idea: Works entirely on opensourced models and frameworks. You dont spend a penny on any of the multimodal features and everything is driven on a system prompt you set!
The gemini integration is a plus in case you need that extra juice but right now in testing I am sticking with gemini for reliability and lack of computing resources. 

